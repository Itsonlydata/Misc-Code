class kt(object):
    
    '''
    bkt contains an assortment of functions for fitting a bayesian 
    knowledge tracing model to a set of learning transaction data. Developed
    originally by Al Corbett and John Anderson (1995), the bayesian knowledge
    tracing model infers student knowledge from a pattern of correct and 
    incorrect answers generated by a learning system, such as an ITS.
    
    bkt.bf is a brute force, or grid search, implementation of the bkt model. 
    It exhaustively searches all possible parameters for the model, identifying
    the best fit model based on minimized sum of squares. Note that, while BF
    optimization is guaranteed to produce the best fit to the data, it takes 
    an impressively long amount of time to run on any single machine (on the
    order of multiple days for students counts in the tens of thousands).
    
    bkt.opt is a gradient descent implementation of the bkt model using the 
    function minimization library from SCiPy. Rather than using an exhaustive 
    search, it uses random starts coupled with a function optimization 
    algorithm to attempt to fit the model. gd runs significantly faster than 
    BF, at the expense of slightly decreased accuracy. Details on the specific
    reduction of accuracy associated with BF vs. OPT can be found in 
    <forthcoming paper>.
    
    bkt.fit is the optimization algorithm used to measure model performance, 
    but can also be used to describe the fit of any given set of parameters, or
    to apply calculated p(ln) and p(corr) calculations to a set of data.
    
    Some basic naming principles: _n class attributes are line-by-line
    parameters. They should not be global variables anywhere where this
    matters,so be careful if you decide to use them in some extension somewhere.
    There's likely a better way to handle how this works, it's just not
    implemented yet.
    '''
    def __init__(self,data,student="student",skill="skill",
                 order="order",correct="correct",debug=False):
        
        from operator import itemgetter
        self.data = data 
        self.header = [x.lower() for x in self.data[0]]
        del self.data[0]
        # deleting the header row
        
        self.student = self.header.index(student.lower())
        self.skill = self.header.index(skill.lower())
        self.order = self.header.index(order.lower())
        self.correct = self.header.index(correct.lower())
        # setting index positions for relevant variables
        
        self.skillids = list(set([x[self.skill] for x in self.data]))
        # generating a list of the skill IDs present in the data
        
        self.data.sort(key=itemgetter(self.student,self.skill,self.order))
        # sorting the data by ordering variable, then by skill, then by student
        
        self.ln_n = None
        self.lprev_n = None
        self.corr_n = None
        self.params_n = None
        # initializing a bunch of attributes for later
        
        self.debug = False
        if debug:
            self.debug = True
        # some debug parameters
    
    def __iter__(self):
        '''
        Allows the BKT object to function as an iterable.
        '''
        return iter(self.data)
        
    def calc_pcorr(self,report=False):
        '''
        Computes the probability that a student will answer the next 
        question correctly, given their current knowledge state. function is
        the probability that student is in the learned state and does not make
        a careless error, plus the probability that the student is in the 
        unlearned state but correctly guesses.
        '''
        return (self.lprev_n*(1-self.params_n[2])) + \
               ((1-self.lprev_n)*self.params_n[1])
    
    def calc_error(self,pred):
        '''
        Calculates the squared error associated with a given correctness
        prediction.
        '''
        return (pred-self.corr_n)**2
    
    def calc_stdev(self):
        import numpy as np
        '''
        Calculates the average and standard deviation of the difference between
        the model prediction and observed student response.
        '''
        obs = [x[self.header.index('correct')] for x in self.data]
        pred = [y[self.header.index('p(ln)')] for y in self.data]
        
        diff = [i-j for i,j in zip(obs,pred)]
        self.ave = np.mean(diff) 
        self.stdev = np.std(diff)
        return
    
    def ln_update(self):
        '''
        Given the p(Ln-1), the fitted bkt parameters, and the student's 
        correct/incorrect answer, determines the updated p(ln) for the 
        current opportunity to practice the skill.
        '''
        
        g = self.params_n[1]
        s = self.params_n[2]
        t = self.params_n[3]
        
        mns = self.lprev_n*(1-s)      # in mastery state, no slip
        ms = self.lprev_n*s           # in mastery state, slip
        nmg = (1-self.lprev_n)*g      # in no mastery state, correct guess
        nmng = (1-self.lprev_n)*(1-g) # in no mastery state, incorrect guess
        nc = 1-self.corr_n            # not correct
        
        ca = float(mns)/float((mns+nmg)) # contribution of a correct answer
        ica = float(ms)/float((ms+nmng)) # contribution of incorrect answer
        new = ((self.corr_n*ca) + (nc*ica))  # intermediate p(ln)
        
        if self.debug:
            self.corr_n_fc = 1
            newfc = ((self.corr_n_fc*ca) + ((1-self.corr_n_fc)*ica))
            self.corr_n_fi = 0
            newfi = ((self.corr_n_fi*ca) + ((1-self.corr_n_fi)*ica))
            
            return [new + ((1-new)*t),
                    newfc + ((1-newfc)*t),
                    newfi + ((1-newfi)*t)]
        
        return new + ((1-new)*t) # the updated p(ln) given answer and p(t)
                
    def fit(self,params,subset):
        '''
        Fits a single skill - you need to pass each skill separately to this.
        '''
        
        self.params_n = params
        st = subset[0][self.student]
        self.lprev_n = self.params_n[0]
        self.ln_n = self.params_n[0]
        error = 0
        
        for l in subset:
            self.corr_n = int(l[self.correct])
            if l[self.student] != st:
                self.lprev_n = self.params_n[0]
            error += self.calc_error(self.calc_pcorr())
            self.ln_n = self.ln_update()
            st = l[self.student]
            self.lprev_n = self.ln_n
        return error
    
    def bf(self,gmax=50,smax=50):
        '''
        Brute force implementation of Bayesian Knowledge Tracing. Exhaustively
        fits the model space to find the best model, to within 0.01.
        Prohibitively slow. Here for completeness, not for any practical use.
        '''
        import itertools
        
        lzero_range = [x/100.0 for x in xrange(1,101)]
        g_range = [x/100.0 for x in xrange(1,gmax+1)]
        s_range = [x/100.0 for x in  xrange(1,smax+1)]
        t_range = [x/100.0 for x in xrange(1,101)]
        pspace = list(itertools.product(lzero_range,g_range,s_range,t_range))
        
        bestmodels = {} # an index of the best model parameters per skill
        
        for x in self.skillids:
            sksubset = [r for r in self.data if r[self.skill] == x]
            best_model_i = None # for tracking the best-performing model
            
            for pset in pspace:
                error = 0
                
                c_stud = sksubset[0][self.student] # the first student
                prevln = pset[0] # initial knowledge
                
                for z in sksubset:
                    if c_stud != z[self.student]: # same student or not?
                        prevln = pset[0]
                
                ln = self.ln_update(prevln,pset,z[self.correct])
                error += self.calc_error(self.calc_pcorr(ln,pset),z[self.correct])**.5
                
                if not best_model_i:
                    best_model_i = list(pset).append(error)
                elif error < best_model_i[-1]:
                    best_model_i = list(pset).append(error)
            bestmodels[x] = best_model_i
        self.bestmodels = bestmodels
    
    def opt(self,nstart,gmax=.50,smax=.50):
        '''
        Function optimization variant of Bayesian Knowledge Tracing model
        fitting. Significantly improved performance over brute force fitting.
        Produces dict[skill_id]: [[lzero,g,s,t],SSR]
        '''
        import random
        import sys
        from scipy.optimize import minimize
        
        modelres = {}
        
        b = [(0.01,1),(0.01,gmax),(0.01,smax),(0.01,1)]
        
        def set_seeds(nstart,gmax,smax):
            pspace = {}
            for _ in xrange(nstart):
                seed = random.randint(0,sys.maxint)
                random.seed(seed)
                lzero = random.random()
                g = random.uniform(0,0.5)
                s = random.uniform(0,0.5)
                t = random.random()
                if g == 0:
                    g = 0.01
                if s == 0:
                    s = 0.01
                pspace[seed] = [lzero,g,s,t]
            return pspace
                
        pspace = set_seeds(nstart,gmax,smax)
        bestmod = ['model',sys.maxint]
        methods = ["Nelder-Mead","Powell","CG","BFGS","Newton-CG",
                   "L-BFGS-B","TNC","COBYLA","SLSQP","dogleg","trust-ncg"]
        
        for sk in self.skillids:
            subset = [i for i in self.data if i[self.skill] == sk]
#             trueparams = [0.25,0.1,0.1,0.15]
#             self.params_n = trueparams
#             self.truefit = self.fit(trueparams,subset)
            for p in pspace:
                self.params_n = pspace[p]
                mdl = minimize(self.fit,self.params_n,args=(subset,),
                               method="L-BFGS-B",bounds = b)
                if mdl['fun'] < bestmod[1]:
                    bestmod = [map(float,mdl['x']),mdl['fun']]
            modelres[sk] = bestmod
        self.bestmodels = modelres

    def apply_params(self,fname,params=None):
        import csv
        '''
        Applies model parameters, either from a fitted model (automatically) or
        from provided parameters (in d[skill]:[params] format). Writes a file
        as output, including the four model parameters, per skill, p(ln), and
        p(corr). Eventually I'll reconfigure this so that it produces an array
        as well, but that's not quite yet.
        '''
            
        fname = fname.strip(".csv")
        writer = csv.writer(open(fname+"_paramswritten.csv","wb"))
        self.header.extend(['lzero','g','s','t','p(ln)','p(ln) forced correct',
                            'p(ln) forced incorrect','p(corr)']) ## take ln_fc and ln_fi out
        writer.writerow(self.header)
        
        if params:
            self.params_n = params[self.data[0][self.skill]]
        else:
            self.params_n = self.bestmodels[self.data[0][self.skill][0]]
            
        self.lprev_n = self.params_n[0]
        st = self.data[0][self.student]
        sk = self.data[0][self.skill]
        
        for d in self.data:
            if sk != d[self.skill]:
                if params:
                    self.params_n = params[d[self.skill]]
                else:
                    self.params_n = self.bestmodels[d[self.skill]][0]
            if st != d[self.student]:
                self.lprev_n = self.params_n[0]
            self.corr_n = int(d[self.correct])
            self.debug = True ## remove this
            self.ln_span = self.ln_update() ## remove this
            self.debug = False ## remove this
            self.ln_n = self.ln_update()
            d.extend([x for x in self.params_n])
            d.extend([self.ln_n,self.ln_span[1],self.ln_span[2],self.calc_pcorr()]) ## remove ln_span
            writer.writerow(d)
            self.lprev_n = self.ln_n
            st = d[self.student]
            sk = d[self.skill]
        return
                
        

'''
Created on Mar 29, 2017

@author: stefanslater
'''
